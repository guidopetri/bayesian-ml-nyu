{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import loadmat\n",
    "import numpy as np\n",
    "\n",
    "# load dataset\n",
    "dataset_1 = loadmat('occam1.mat')\n",
    "dataset_2 = loadmat('occam2.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15, 1)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_1['x'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 1)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_2['x'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the gradients according to 1c\n",
    "def compute_gradients(X, y, alpha_2, sigma_2, phi, z=None):\n",
    "    # return gradient_z, gradient_alpha, gradient_sigma\n",
    "    # precalculate phi(X, z) so we don't have to calculate it several times\n",
    "    phi_xz = phi(X, z)\n",
    "    phi_t = phi_xz.T\n",
    "    N, k = phi_xz.shape\n",
    "    \n",
    "    # calculate Sigma since we also use it several times\n",
    "    Sigma = (1 / alpha_2) * np.identity(k) + (1 / sigma_2) * phi_t @ phi_xz\n",
    "    \n",
    "    # add jitter, according to hint 3\n",
    "    jitter_amount = 1e-6\n",
    "    Sigma = Sigma + jitter_amount * np.identity(k)\n",
    "    \n",
    "    # invert\n",
    "    sigma_inv = np.linalg.inv(Sigma)\n",
    "\n",
    "    # default initialization\n",
    "    gradient_z = 0\n",
    "    \n",
    "    if phi.z_shape != 0:\n",
    "        gradient_z = []\n",
    "        # for each dimension of z\n",
    "        for i in range(len(z)):\n",
    "            # take the derivative with respect to that dimension\n",
    "            phi_derivative = phi.z_derivative(X.squeeze(), z)[i]\n",
    "            phi_t_derivative = phi.z_derivative(X.squeeze(), z)[i].T\n",
    "\n",
    "            sigma_derivative = (phi_t_derivative @ phi_xz + phi_t @ phi_derivative) / sigma_2\n",
    "\n",
    "            # calculate gradient\n",
    "            gradient_z_i = 0.5 * (- np.trace(sigma_inv @ sigma_derivative)\n",
    "                                  + (y.T @ phi_derivative @ sigma_inv @ phi_t @ y\n",
    "                                     - y.T @ phi_xz @ sigma_inv @ sigma_derivative @ sigma_inv @ phi_t @ y\n",
    "                                     + y.T @ phi_xz @ sigma_inv @ phi_t_derivative @ y\n",
    "                                     ) / (sigma_2 ** 2)\n",
    "                                  )\n",
    "            gradient_z.append(gradient_z_i.item())\n",
    "\n",
    "    # calculate alpha and sigma gradient according to 1c\n",
    "    gradient_alpha = 0.5 * (- k / alpha_2\n",
    "                            + np.trace(sigma_inv) / (alpha_2 ** 2)\n",
    "                            + y.T @ phi_xz @ sigma_inv @ sigma_inv @ phi_t @ y / (sigma_2 ** 2 * alpha_2 ** 2))\n",
    "    \n",
    "    gradient_sigma = - 0.5 * (N / sigma_2\n",
    "                              - np.trace(sigma_inv @ phi_t @ phi_xz) / (sigma_2 ** 2)\n",
    "                              - y.T @ y / (sigma_2 ** 2)\n",
    "                              - y.T @ phi_xz @ (sigma_inv @ phi_t @ phi_xz @ sigma_inv / sigma_2\n",
    "                                                - 2 * sigma_inv) @ phi_t @ y / (sigma_2 ** 3)\n",
    "                              )\n",
    "    # return list, float, float\n",
    "    return gradient_z, gradient_alpha.item(), gradient_sigma.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradient checker function: perturb a single dimension by epsilon and measure the difference\n",
    "def gradient_checker(gradient, objective_func, X, y, alpha, sigma, phi, z, check_variable,\n",
    "                     idx_z_check=0, epsilon=1e-4, tolerance=1e-2):\n",
    "    if check_variable == 'alpha':\n",
    "        approx_gradient = (objective_func(X, y, alpha + epsilon, sigma, phi, z)\n",
    "                           - objective_func(X, y, alpha - epsilon, sigma, phi, z)\n",
    "                           ) / (2 * epsilon)\n",
    "    elif check_variable == 'sigma':\n",
    "        approx_gradient = (objective_func(X, y, alpha, sigma + epsilon, phi, z)\n",
    "                           - objective_func(X, y, alpha, sigma - epsilon, phi, z)\n",
    "                           ) / (2 * epsilon)\n",
    "    elif check_variable == 'z':\n",
    "        approx_gradient = (objective_func(X, y, alpha, sigma, phi, z + np.array([epsilon * (idx == idx_z_check)\n",
    "                                                                                 for idx in range(len(z))]))\n",
    "                           - objective_func(X, y, alpha, sigma, phi, z - np.array([epsilon * (idx == idx_z_check)\n",
    "                                                                                   for idx in range(len(z))]))\n",
    "                           ) / (2 * epsilon)\n",
    "    # check the norm of the distance between the perturbed, numerical gradient and the analytical gradient\n",
    "    distance = np.linalg.norm(gradient - approx_gradient.item())\n",
    "    if distance > tolerance:\n",
    "        raise ValueError(f'Gradient did not match for parameters: alpha={alpha}, '\n",
    "                         f'sigma={sigma}, phi={phi}, z={z}, gradient={gradient}, '\n",
    "                         f'approx_gradient={approx_gradient.item()}, variable={check_variable}'\n",
    "                         )\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the \"original\" version of 1b, without caring about dimensionality\n",
    "def calc_log_marginal_likelihood_old(X, y, alpha_2, sigma_2, phi, z=None):\n",
    "    N = X.shape[0]\n",
    "    # phi @ phi.T yields (N,N) shape matrix, which doesn't work for dataset 2\n",
    "    Sigma = sigma_2 * np.identity(N) + alpha_2 * phi(X, z) @ phi(X, z).T\n",
    "    \n",
    "    # add jitter, according to hint 3\n",
    "    jitter_amount = 1e-6\n",
    "    Sigma = Sigma + jitter_amount * np.identity(N)\n",
    "\n",
    "    # LML terms\n",
    "    first = - N * np.log(2 * np.pi) / 2\n",
    "    second = - np.log(np.linalg.det(Sigma)) / 2\n",
    "    third = - y.T @ np.linalg.inv(Sigma) @ y / 2\n",
    "\n",
    "    return first + second + third"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1b with better dimensionality\n",
    "def calc_log_marginal_likelihood(X, y, alpha_2, sigma_2, phi, z=None):\n",
    "    # precalculation to avoid repeated evaluation of phi\n",
    "    phi_xz = phi(X, z)\n",
    "    N, phi_shape = phi_xz.shape\n",
    "    phi_t = phi_xz.T\n",
    "    # sigma is now (param_count_phi, param_count_phi) shape\n",
    "    Sigma = (1 / alpha_2) * np.identity(phi_shape) + (1 / sigma_2) * phi_t @ phi_xz\n",
    "    \n",
    "    # add jitter, according to hint 3\n",
    "    jitter_amount = 1e-6\n",
    "    Sigma = Sigma + jitter_amount * np.identity(phi_shape)\n",
    "\n",
    "    # LML terms\n",
    "    first = - N * np.log(2 * np.pi) / 2\n",
    "    second = - N * math.log(alpha_2 ** (phi_shape / N) * sigma_2) / 2\n",
    "    third = - math.log(np.linalg.det(-Sigma)) / 2\n",
    "    fourth = - y.T @ y / (2 * sigma_2)\n",
    "    fifth = y.T @ phi_xz @ np.linalg.inv(Sigma) @ phi_t @ y / (2 * (sigma_2 ** 2))\n",
    "\n",
    "    return first + second + third + fourth + fifth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model_1(object):\n",
    "    # model 1, according to 3a\n",
    "    def __init__(self):\n",
    "        # z doesn't matter\n",
    "        self.z_shape = 0\n",
    "        self.z_derivative = lambda x, z: None\n",
    "    \n",
    "    def __call__(self, x, *args):\n",
    "        # calling model_1(X, z) returns 1, x, x^2, x^3, x^4, x^5\n",
    "        return np.stack([np.ones(x.shape),\n",
    "                         x,\n",
    "                         x ** 2,\n",
    "                         x ** 3,\n",
    "                         x ** 4,\n",
    "                         x ** 5],\n",
    "                        ).squeeze().T\n",
    "\n",
    "class Model_2(object):\n",
    "    def __init__(self):\n",
    "        # z has two elements\n",
    "        self.z_shape = 2\n",
    "    \n",
    "    def z_derivative(self, x, z):\n",
    "        # derivatives of phi(x, z) with respect to z, for each element of z\n",
    "        # this is technically a tensor, but is implemented as a list of matrices\n",
    "        # shaped (2, N, 2)\n",
    "        return [np.stack([2 * (x - 1) ** 2 * np.exp(- ((x - 1) ** 2) / (z[0] ** 2)) / (z[0] ** 3),\n",
    "                         np.zeros(x.shape)]).T,\n",
    "                np.stack([np.zeros(x.shape),\n",
    "                          2 * (x - 5) ** 2 * np.exp(- ((x - 5) ** 2) / (z[1] ** 2)) / (z[1] ** 3),]).T\n",
    "                ]\n",
    "    \n",
    "    def __call__(self, x, z):\n",
    "        # phi_2 (X, z)\n",
    "        return np.stack([np.exp(- ((x - 1) ** 2 / z[0] ** 2)),\n",
    "                         np.exp(- ((x - 5) ** 2 / z[1] ** 2))],\n",
    "                         ).squeeze().T\n",
    "\n",
    "class Model_Test(object):\n",
    "    # for testing purposes\n",
    "    def __init__(self):\n",
    "        self.z_shape = 2\n",
    "    \n",
    "    def z_derivative(self, x, z):\n",
    "        # (2, N, 2) shape tensor\n",
    "        return [np.stack([x, np.zeros(x.shape)]).T,\n",
    "                np.stack([np.zeros(x.shape), x ** 2]).T\n",
    "                ]\n",
    "\n",
    "    def __call__(self, x, z):\n",
    "        # simple functions to make sure the derivatives for model 2 were not the problem\n",
    "        return np.stack([x * z[0], z[1] * x ** 2]).squeeze().T\n",
    "    \n",
    "class Model_3(object):\n",
    "    def __init__(self):\n",
    "        # z doesn't matter again\n",
    "        self.z_shape = 0\n",
    "        self.z_derivative = lambda x, z: None\n",
    "    \n",
    "    def __call__(self, x, *args):\n",
    "        # phi_3(X, z)\n",
    "        return np.stack([x,\n",
    "                         np.cos(2 * x)],\n",
    "                        ).squeeze().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the models\n",
    "model_1 = Model_1()\n",
    "model_2 = Model_2()\n",
    "model_3 = Model_3()\n",
    "model_test = Model_Test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, -0.043093644619835, 0.3305149266941671)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# small test for compute_gradients to see if it runs\n",
    "compute_gradients(dataset_1['x'],\n",
    "                  dataset_1['y'],\n",
    "                  13.79,\n",
    "                  213.44,\n",
    "                  model_1,\n",
    "                  z=None,\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-4757.08427572]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# same for calculating LML\n",
    "calc_log_marginal_likelihood(dataset_1['x'], dataset_1['y'], 4, 3, model_1, [1, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# checking if gradient checker works\n",
    "gradient_checker(-201.8404, calc_log_marginal_likelihood,\n",
    "                 dataset_1['x'], dataset_1['y'], 4, 3, model_2, np.array([1, 2]), 'z', idx_z_check=0, epsilon=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch gradient ascent code adapted from my 1003 code\n",
    "def batch_grad_ascent(X, y, step_size_alpha=0.1, step_size_sigma=1, step_size_z=0.1,\n",
    "                       phi=None, grad_check=False):\n",
    "\n",
    "    # initialization procedures\n",
    "    likelihood_history = []\n",
    "    alpha_history = [1]\n",
    "    sigma_history = [1]\n",
    "    num_instances, num_features = X.shape[0], 1\n",
    "    z = np.ones(phi.z_shape)\n",
    "    alpha = 500\n",
    "    sigma = 500\n",
    "    iteration = 0\n",
    "    \n",
    "    # start the LML here\n",
    "    log_marginal_likelihood = calc_log_marginal_likelihood(X, y, alpha, sigma, phi, z)\n",
    "    likelihood_history.append(log_marginal_likelihood.item())\n",
    "    \n",
    "    if phi is None:\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    # while not converged\n",
    "    while True:\n",
    "        iteration += 1\n",
    "        # compute the gradients\n",
    "        gradient_z, gradient_alpha, gradient_sigma = compute_gradients(X, y, alpha, sigma, phi, z)\n",
    "        if grad_check:\n",
    "            gradient_checker(gradient_alpha, calc_log_marginal_likelihood, X, y, alpha, sigma, phi, z, check_variable='alpha')\n",
    "            gradient_checker(gradient_sigma, calc_log_marginal_likelihood, X, y, alpha, sigma, phi, z, check_variable='sigma')\n",
    "            gradient_checker(gradient_z, calc_log_marginal_likelihood, X, y, alpha, sigma, phi, z, check_variable='z')\n",
    "        # step in the gradient direction (ascent, not descent)\n",
    "        # ensure that we do not go beyond 0 for alpha/sigma\n",
    "        z = [z[i] + step_size_z * gradient_z[i] for i in range(len(z))]\n",
    "        alpha = max(1e-16, alpha + step_size_alpha * gradient_alpha)\n",
    "        sigma = max(1e-16, sigma + step_size_sigma * gradient_sigma)\n",
    "        \n",
    "        # calculate the updated LML\n",
    "        log_marginal_likelihood = calc_log_marginal_likelihood(X, y, alpha, sigma, phi, z)\n",
    "        \n",
    "        if iteration % 10000 == 0:\n",
    "            # anneal every 10k iterations\n",
    "            step_size_alpha *= 0.9\n",
    "            step_size_sigma *= 0.9\n",
    "            step_size_z *= 0.9\n",
    "            print(f'At iteration {iteration}, diff size: {abs(likelihood_history[-1] - log_marginal_likelihood)}')\n",
    "        \n",
    "        if abs(likelihood_history[-1] - log_marginal_likelihood) < 1e-5:\n",
    "            # if converged to 1e-3, exit loop\n",
    "            break\n",
    "            \n",
    "        likelihood_history.append(log_marginal_likelihood.item())\n",
    "        alpha_history.append(alpha)\n",
    "        sigma_history.append(sigma)\n",
    "        \n",
    "        if np.isnan(log_marginal_likelihood.item()):\n",
    "            # if LML is NaN, then something went wrong. time to investigate\n",
    "            print('Something broke')\n",
    "            return z, alpha, sigma, likelihood_history, alpha_history, sigma_history\n",
    "    # return everything we need\n",
    "    return z, alpha, sigma, likelihood_history, alpha_history, sigma_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At iteration 10000, diff size: [[1.71910593e-05]]\n",
      "At iteration 20000, diff size: [[1.49989901e-05]]\n",
      "At iteration 30000, diff size: [[1.31346948e-05]]\n",
      "At iteration 40000, diff size: [[1.15388401e-05]]\n",
      "At iteration 50000, diff size: [[1.01646469e-05]]\n",
      "Model 0, dataset 0, likelihood: -112.32776967653916\n",
      "alpha 500.3169853759835, sigma 516.6810441912883, z []\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'sys' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-45-15c73348211c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'Model {model_idx}, dataset {dataset_idx}, likelihood: {likelihood_history[-1]}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'alpha {alpha_hat}, sigma {sigma_hat}, z {z_hat}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'sys' is not defined"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "# for each model\n",
    "for model_idx, model in enumerate([model_1, model_2, model_3]):\n",
    "    # for each dataset\n",
    "    for dataset_idx, dataset in enumerate([dataset_1, dataset_2]):\n",
    "        # run the gradient ascent\n",
    "        z_hat, alpha_hat, sigma_hat, likelihood_history, alpha_history, sigma_history = batch_grad_ascent(dataset['x'],\n",
    "                                                                             dataset['y'],\n",
    "                                                                             step_size_alpha=(dataset_idx + 1) * 1e-2,\n",
    "                                                                             step_size_sigma=(dataset_idx + 1) * 1e-2,\n",
    "                                                                             step_size_z=(dataset_idx + 1) * 1e-2,\n",
    "                                                                             phi=model,\n",
    "                                                                                                          grad_check=True\n",
    "                                                                             )\n",
    "        results.append((z_hat, alpha_hat, sigma_hat, likelihood_history[-1], alpha_history[-1], sigma_history[-1]))\n",
    "        print(f'Model {model_idx}, dataset {dataset_idx}, likelihood: {likelihood_history[-1]}')\n",
    "        print(f'alpha {alpha_hat}, sigma {sigma_hat}, z {z_hat}')\n",
    "        sys.exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2.2968441361569205, 0.0028246991695384444)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 / 0.43538, 1 / 354.02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: 1, dataset: 1, z_hat: [], alpha_hat: 13.785646995772991, sigma_hat: 213.43969760634997, maximal LML: -149.65952399031198\n",
      "\n",
      "model: 1, dataset: 2, z_hat: [], alpha_hat: 1e-16, sigma_hat: 2514048.4860248924, maximal LML: -828814.2872547597\n",
      "\n",
      "model: 2, dataset: 1, z_hat: [0.722236917568794, 13.287302801395171], alpha_hat: 30.939916546731464, sigma_hat: 222.9437035677023, maximal LML: -136.53651510058575\n",
      "\n",
      "model: 2, dataset: 2, z_hat: [-240.66518582774646, 335.0918319322162], alpha_hat: 0.9886313976363109, sigma_hat: 2522297.4309178446, maximal LML: -828977.9435626994\n",
      "\n",
      "model: 3, dataset: 1, z_hat: [], alpha_hat: 41.47462339809696, sigma_hat: 213.0427744583523, maximal LML: -135.3950607612474\n",
      "\n",
      "model: 3, dataset: 2, z_hat: [], alpha_hat: 1e-16, sigma_hat: 2504685.7515116213, maximal LML: -828627.9195774083\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print best values separately from the running code\n",
    "[print(f'model: {1 + idx // 2}, dataset: {1 + idx % 2}, z_hat: {z},'\n",
    "       f' alpha_hat: {a}, sigma_hat: {s}, maximal LML: {l}\\n')\n",
    " for idx, (z, a, s, l, _, _) in enumerate(results)];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, dataset 0, evidence: 5.2757201689290716e-74\n",
      "Model 0, dataset 1, evidence: 0.0\n",
      "Model 1, dataset 0, evidence: 1.5675635719594354e-55\n",
      "Model 1, dataset 1, evidence: 0.0\n",
      "Model 2, dataset 0, evidence: 7.772080433634596e-54\n",
      "Model 2, dataset 1, evidence: 0.0\n"
     ]
    }
   ],
   "source": [
    "evidences = []\n",
    "\n",
    "# calculate evidences for each model/dataset combo\n",
    "for model_idx, model in enumerate([model_1, model_2, model_3]):\n",
    "    for dataset_idx, dataset in enumerate([dataset_1, dataset_2]):\n",
    "        # parameter count matters for the identity matrices\n",
    "        if model_idx == 0:\n",
    "            param_count = 6\n",
    "        else:\n",
    "            param_count = 2\n",
    "\n",
    "        N = dataset['x'].shape[0]\n",
    "        z_hat, alpha_hat, sigma_hat, likelihood_history, alpha_history, sigma_history = results[model_idx * 2 + dataset_idx]\n",
    "\n",
    "        # calculate log evidence because some of these numbers are too small for python to handle\n",
    "        log_evidence = (likelihood_history\n",
    "                        # parameter complexity\n",
    "                        + 0.5 * param_count * np.log(2 * np.pi)\n",
    "                        # prior\n",
    "                        + N * np.log((2 * np.pi) ** (-param_count / (2 * N)) * alpha_hat)\n",
    "                        + 0\n",
    "                        # rewriting A for tractability using (24) from matrix cookbook\n",
    "                        - 0.5 * N * np.log(sigma_hat)\n",
    "                        - 0.5 * param_count * np.log(alpha_hat)\n",
    "                        - 0.5 * np.log(\n",
    "                            np.linalg.det(- (1 / alpha_hat) * np.identity(param_count)\n",
    "                                          - (1 / sigma_hat) * model(dataset['x'], z_hat).T @ model(dataset['x'], z_hat)\n",
    "                                         )))\n",
    "\n",
    "        evidence = np.exp(log_evidence)  # becomes 0 for the second dataset because the evidence is so low...\n",
    "        print(f'Model {model_idx}, dataset {dataset_idx}, evidence: {evidence}')\n",
    "        evidences.append((model_idx, dataset_idx, log_evidence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(evidences, columns=['Model index', 'Dataset index', 'Evidence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 936x504 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(13, 7))\n",
    "\n",
    "plt.axes(axs[0])\n",
    "df[df['Dataset index'] == 0]['Evidence'].plot.bar()\n",
    "plt.title('Dataset 1')\n",
    "plt.xlabel('Model index')\n",
    "plt.xticks(ticks=[0, 1, 2], labels=[0, 1, 2])\n",
    "plt.ylabel('Log evidence (higher is better)')\n",
    "\n",
    "plt.axes(axs[1])\n",
    "df[df['Dataset index'] == 1]['Evidence'].plot.bar()\n",
    "plt.title('Dataset 2')\n",
    "plt.xlabel('Model index')\n",
    "plt.xticks(ticks=[0, 1, 2], labels=[0, 1, 2])\n",
    "plt.ylabel('Log evidence (higher is better)');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3c\n",
    "\n",
    "The model with the highest evidence on the second dataset is the second model (index 1), with\n",
    "\n",
    "$$\n",
    "\\phi(x, \\textbf{z}) = (\\exp(- \\dfrac{(x-1)^2}{z_1^2}), \\exp(- \\dfrac{(x-5)^2}{z_2^2}))^T\n",
    "$$\n",
    "\n",
    "and $\\textbf{z} = (z_1, z_2)^T$. The posterior mean and variance can be calculated using Bishop, 3.49 (with 3.50 and 3.51):\n",
    "\n",
    "$$\n",
    "p(\\textbf{w} | \\mathcal{D}) = \\mathcal{N}(\\textbf{w}| m_N, S_N)\n",
    "$$\n",
    "\n",
    "with\n",
    "\n",
    "$$\n",
    "m_N = S_N (S_0^{-1} m_0 + \\beta \\Phi^T \\textbf{t}\n",
    "$$\n",
    "\n",
    "$$\n",
    "S_N^{-1} = S_0^{-1} + \\beta \\Phi^T \\Phi\n",
    "$$\n",
    "\n",
    "Thus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Posterior mean: \n",
      "[[0.07250421]\n",
      " [0.07255212]], \n",
      "\n",
      "posterior variance: \n",
      "[[1.05111259 0.03962688]\n",
      " [0.03962688 1.05113985]]\n"
     ]
    }
   ],
   "source": [
    "z_hat, alpha_hat, sigma_hat, likelihood_history, alpha_history, sigma_history = results[3]\n",
    "phi_xz = model_2(dataset_2['x'], z_hat)\n",
    "\n",
    "prior_mean = np.zeros((2, 1))\n",
    "S_N = (1 / alpha_hat) * np.identity(2) + (1 / sigma_hat) * phi_xz.T @ phi_xz\n",
    "m_N = np.linalg.inv(S_N) @ ((1 / alpha_hat) * np.identity(2) @ prior_mean + (1 / sigma_hat) * phi_xz.T @ dataset_2['y'])\n",
    "\n",
    "print(f'Posterior mean: \\n{m_N}, \\n\\nposterior variance: \\n{S_N}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
