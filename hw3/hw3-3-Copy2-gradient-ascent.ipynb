{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import loadmat\n",
    "import numpy as np\n",
    "\n",
    "# load dataset\n",
    "dataset_1 = loadmat('occam1.mat')\n",
    "dataset_2 = loadmat('occam2.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1070,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15, 1)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_1['x'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 1)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_2['x'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradient checker function: perturb a single dimension by epsilon and measure the difference\n",
    "def gradient_checker(gradient, objective_func, X, y, alpha, sigma, phi, z, check_variable,\n",
    "                     idx_z_check=0, epsilon=1e-4, tolerance=1e-4):\n",
    "    if check_variable == 'alpha':\n",
    "        approx_gradient = (objective_func(X, y, alpha + epsilon, sigma, phi, z)\n",
    "                           - objective_func(X, y, alpha - epsilon, sigma, phi, z)\n",
    "                           ) / (2 * epsilon)\n",
    "    elif check_variable == 'sigma':\n",
    "        approx_gradient = (objective_func(X, y, alpha, sigma + epsilon, phi, z)\n",
    "                           - objective_func(X, y, alpha, sigma - epsilon, phi, z)\n",
    "                           ) / (2 * epsilon)\n",
    "    elif check_variable == 'z':\n",
    "        approx_gradient = (objective_func(X, y, alpha, sigma, phi, z + np.array([epsilon * (idx == idx_z_check)\n",
    "                                                                                 for idx in range(len(z))]))\n",
    "                           - objective_func(X, y, alpha, sigma, phi, z - np.array([epsilon * (idx == idx_z_check)\n",
    "                                                                                   for idx in range(len(z))]))\n",
    "                           ) / (2 * epsilon)\n",
    "    # check the norm of the distance between the perturbed, numerical gradient and the analytical gradient\n",
    "    distance = np.linalg.norm(gradient - approx_gradient.item())\n",
    "    if distance > tolerance:\n",
    "        raise ValueError(f'Gradient did not match for parameters: alpha={alpha}, '\n",
    "                         f'sigma={sigma}, phi={phi}, z={z}, gradient={gradient}, '\n",
    "                         f'approx_gradient={approx_gradient.item()}, variable={check_variable}'\n",
    "                         )\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1393,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the gradients according to 1c\n",
    "def compute_gradients(X, y, alpha_2, sigma_2, phi, z=None):\n",
    "    # return gradient_z, gradient_alpha, gradient_sigma\n",
    "    # precalculate phi(X, z) so we don't have to calculate it several times\n",
    "    phi_xz = phi(X, z)\n",
    "    phi_t = phi_xz.T\n",
    "    N, k = phi_xz.shape\n",
    "    \n",
    "    # calculate Sigma since we also use it several times\n",
    "    Sigma = (1 / alpha_2) * np.identity(k) + (1 / sigma_2) * phi_t @ phi_xz\n",
    "    \n",
    "    # add jitter, according to hint 3\n",
    "    jitter_amount = 1e-6\n",
    "    Sigma = Sigma + jitter_amount * np.identity(k)\n",
    "    \n",
    "    # invert\n",
    "    sigma_inv = np.linalg.inv(Sigma)\n",
    "\n",
    "    # default initialization\n",
    "    gradient_z = 0\n",
    "    \n",
    "    if phi.z_shape != 0:\n",
    "        gradient_z = []\n",
    "        # for each dimension of z\n",
    "        for i in range(len(z)):\n",
    "            # take the derivative with respect to that dimension\n",
    "            phi_derivative = phi.z_derivative(X.squeeze(), z)[i]\n",
    "            phi_t_derivative = phi.z_derivative(X.squeeze(), z)[i].T\n",
    "\n",
    "            sigma_derivative = (phi_t_derivative @ phi_xz + phi_t @ phi_derivative) / sigma_2\n",
    "\n",
    "            # calculate gradient\n",
    "            gradient_z_i = 0.5 * (- np.trace(sigma_inv @ sigma_derivative)\n",
    "                                  + (y.T @ phi_derivative @ sigma_inv @ phi_t @ y\n",
    "                                     - y.T @ phi_xz @ sigma_inv @ sigma_derivative @ sigma_inv @ phi_t @ y\n",
    "                                     + y.T @ phi_xz @ sigma_inv @ phi_t_derivative @ y\n",
    "                                     ) / (sigma_2 ** 2)\n",
    "                                  )\n",
    "            gradient_z.append(gradient_z_i.item())\n",
    "\n",
    "    # calculate alpha and sigma gradient according to 1c\n",
    "    gradient_alpha = 0.5 * (- k / alpha_2\n",
    "                            + np.trace(sigma_inv) / (alpha_2 ** 2)\n",
    "                            + y.T @ phi_xz @ sigma_inv @ sigma_inv @ phi_t @ y / (sigma_2 ** 2 * alpha_2 ** 2))\n",
    "    \n",
    "    gradient_sigma = - 0.5 * (N / sigma_2\n",
    "                              - np.trace(sigma_inv @ phi_t @ phi_xz) / (sigma_2 ** 2)\n",
    "                              - y.T @ y / (sigma_2 ** 2)\n",
    "                              - y.T @ phi_xz @ (sigma_inv @ phi_t @ phi_xz @ sigma_inv / sigma_2\n",
    "                                                - 2 * sigma_inv) @ phi_t @ y / (sigma_2 ** 3)\n",
    "                              )\n",
    "    # return list, float, float\n",
    "    return gradient_z, gradient_alpha.item(), gradient_sigma.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 852,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the \"original\" version of 1b, without caring about dimensionality\n",
    "def calc_log_marginal_likelihood_old(X, y, alpha_2, sigma_2, phi, z=None):\n",
    "    N = X.shape[0]\n",
    "    # phi @ phi.T yields (N,N) shape matrix, which doesn't work for dataset 2\n",
    "    Sigma = sigma_2 * np.identity(N) + alpha_2 * phi(X, z) @ phi(X, z).T\n",
    "    \n",
    "    # add jitter, according to hint 3\n",
    "    jitter_amount = 1e-6\n",
    "    Sigma = Sigma + jitter_amount * np.identity(N)\n",
    "\n",
    "    # LML terms\n",
    "    first = - N * np.log(2 * np.pi) / 2\n",
    "    second = - np.log(np.linalg.det(Sigma)) / 2\n",
    "    third = - y.T @ np.linalg.inv(Sigma) @ y / 2\n",
    "\n",
    "    return first + second + third"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1230,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1b with better dimensionality\n",
    "def calc_log_marginal_likelihood(X, y, alpha_2, sigma_2, phi, z=None):\n",
    "    # precalculation to avoid repeated evaluation of phi\n",
    "    phi_xz = phi(X, z)\n",
    "    N, phi_shape = phi_xz.shape\n",
    "    phi_t = phi_xz.T\n",
    "    # sigma is now (param_count_phi, param_count_phi) shape\n",
    "    Sigma = (1 / alpha_2) * np.identity(phi_shape) + (1 / sigma_2) * phi_t @ phi_xz\n",
    "    \n",
    "    # add jitter, according to hint 3\n",
    "    jitter_amount = 1e-6\n",
    "    Sigma = Sigma + jitter_amount * np.identity(phi_shape)\n",
    "\n",
    "    # LML terms\n",
    "    first = - N * np.log(2 * np.pi) / 2\n",
    "    second = - N * math.log(alpha_2 ** (phi_shape / N) * sigma_2) / 2\n",
    "    third = - math.log(np.linalg.det(-Sigma)) / 2\n",
    "    fourth = - y.T @ y / (2 * sigma_2)\n",
    "    fifth = y.T @ phi_xz @ np.linalg.inv(Sigma) @ phi_t @ y / (2 * (sigma_2 ** 2))\n",
    "\n",
    "    return first + second + third + fourth + fifth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1387,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model_1(object):\n",
    "    # model 1, according to 3a\n",
    "    def __init__(self):\n",
    "        # z doesn't matter\n",
    "        self.z_shape = 0\n",
    "        self.z_derivative = lambda x, z: None\n",
    "    \n",
    "    def __call__(self, x, *args):\n",
    "        # calling model_1(X, z) returns 1, x, x^2, x^3, x^4, x^5\n",
    "        return np.stack([np.ones(x.shape),\n",
    "                         x,\n",
    "                         x ** 2,\n",
    "                         x ** 3,\n",
    "                         x ** 4,\n",
    "                         x ** 5],\n",
    "                        ).squeeze().T\n",
    "\n",
    "class Model_2(object):\n",
    "    def __init__(self):\n",
    "        # z has two elements\n",
    "        self.z_shape = 2\n",
    "    \n",
    "    def z_derivative(self, x, z):\n",
    "        # derivatives of phi(x, z) with respect to z, for each element of z\n",
    "        # this is technically a tensor, but is implemented as a list of matrices\n",
    "        # shaped (2, N, 2)\n",
    "        return [np.stack([2 * (x - 1) ** 2 * np.exp(- ((x - 1) ** 2) / (z[0] ** 2)) / (z[0] ** 3),\n",
    "                         np.zeros(x.shape)]).T,\n",
    "                np.stack([np.zeros(x.shape),\n",
    "                          2 * (x - 5) ** 2 * np.exp(- ((x - 5) ** 2) / (z[1] ** 2)) / (z[1] ** 3),]).T\n",
    "                ]\n",
    "    \n",
    "    def __call__(self, x, z):\n",
    "        # phi_2 (X, z)\n",
    "        return np.stack([np.exp(- ((x - 1) ** 2 / z[0] ** 2)),\n",
    "                         np.exp(- ((x - 5) ** 2 / z[1] ** 2))],\n",
    "                         ).squeeze().T\n",
    "\n",
    "class Model_Test(object):\n",
    "    # for testing purposes\n",
    "    def __init__(self):\n",
    "        self.z_shape = 2\n",
    "    \n",
    "    def z_derivative(self, x, z):\n",
    "        # (2, N, 2) shape tensor\n",
    "        return [np.stack([x, np.zeros(x.shape)]).T,\n",
    "                np.stack([np.zeros(x.shape), x ** 2]).T\n",
    "                ]\n",
    "\n",
    "    def __call__(self, x, z):\n",
    "        # simple functions to make sure the derivatives for model 2 were not the problem\n",
    "        return np.stack([x * z[0], z[1] * x ** 2]).squeeze().T\n",
    "    \n",
    "class Model_3(object):\n",
    "    def __init__(self):\n",
    "        # z doesn't matter again\n",
    "        self.z_shape = 0\n",
    "        self.z_derivative = lambda x, z: None\n",
    "    \n",
    "    def __call__(self, x, *args):\n",
    "        # phi_3(X, z)\n",
    "        return np.stack([x,\n",
    "                         np.cos(2 * x)],\n",
    "                        ).squeeze().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1388,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the models\n",
    "model_1 = Model_1()\n",
    "model_2 = Model_2()\n",
    "model_3 = Model_3()\n",
    "model_test = Model_Test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1395,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([-201.84040546517545, 171.7267329620542],\n",
       " 92.00499484961402,\n",
       " 1698.2944171554418)"
      ]
     },
     "execution_count": 1395,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# small test for compute_gradients to see if it runs\n",
    "compute_gradients(dataset_1['x'],\n",
    "                  dataset_1['y'],\n",
    "                  4,\n",
    "                  3,\n",
    "                  model_2,\n",
    "                  z=[1, 2]\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1396,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-4757.08427572]])"
      ]
     },
     "execution_count": 1396,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# same for calculating LML\n",
    "calc_log_marginal_likelihood(dataset_1['x'], dataset_1['y'], 4, 3, model_1, [1, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1397,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# checking if gradient checker works\n",
    "gradient_checker(-201.8404, calc_log_marginal_likelihood,\n",
    "                 dataset_1['x'], dataset_1['y'], 4, 3, model_2, np.array([1, 2]), 'z', idx_z_check=0, epsilon=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1404,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch gradient ascent code adapted from my 1003 code\n",
    "def batch_grad_ascent(X, y, step_size_alpha=0.1, step_size_sigma=1, step_size_z=0.1,\n",
    "                       phi=None, grad_check=False):\n",
    "\n",
    "    # initialization procedures\n",
    "    likelihood_history = []\n",
    "    alpha_history = [1]\n",
    "    sigma_history = [1]\n",
    "    num_instances, num_features = X.shape[0], 1\n",
    "    z = np.ones(phi.z_shape)\n",
    "    alpha = 1\n",
    "    sigma = 1\n",
    "    iteration = 0\n",
    "    \n",
    "    # start the LML here\n",
    "    log_marginal_likelihood = calc_log_marginal_likelihood(X, y, alpha, sigma, phi, z)\n",
    "    likelihood_history.append(log_marginal_likelihood.item())\n",
    "    \n",
    "    if phi is None:\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    # while not converged\n",
    "    while True:\n",
    "        iteration += 1\n",
    "        # compute the gradients\n",
    "        gradient_z, gradient_alpha, gradient_sigma = compute_gradients(X, y, alpha, sigma, phi, z)\n",
    "        if grad_check:\n",
    "            gradient_checker(gradient_alpha, calc_log_marginal_likelihood, X, y, alpha, sigma, phi, z, check_variable='alpha')\n",
    "            gradient_checker(gradient_sigma, calc_log_marginal_likelihood, X, y, alpha, sigma, phi, z, check_variable='sigma')\n",
    "            gradient_checker(gradient_z, calc_log_marginal_likelihood, X, y, alpha, sigma, phi, z, check_variable='z')\n",
    "        # step in the gradient direction (ascent, not descent)\n",
    "        # ensure that we do not go beyond 0 for alpha/sigma\n",
    "        z = [z[i] + step_size_z * gradient_z[i] for i in range(len(z))]\n",
    "        alpha = max(1e-16, alpha + step_size_alpha * gradient_alpha)\n",
    "        sigma = max(1e-16, sigma + step_size_sigma * gradient_sigma)\n",
    "        \n",
    "        # calculate the updated LML\n",
    "        log_marginal_likelihood = calc_log_marginal_likelihood(X, y, alpha, sigma, phi, z)\n",
    "        \n",
    "        if iteration % 10000 == 0:\n",
    "            # anneal every 10k iterations\n",
    "            step_size_alpha *= 0.9\n",
    "            step_size_sigma *= 0.9\n",
    "            step_size_z *= 0.9\n",
    "            print(f'At iteration {iteration}, diff size: {abs(likelihood_history[-1] - log_marginal_likelihood)}')\n",
    "        \n",
    "        if abs(likelihood_history[-1] - log_marginal_likelihood) < 1e-3:\n",
    "            # if converged to 1e-3, exit loop\n",
    "            break\n",
    "            \n",
    "        likelihood_history.append(log_marginal_likelihood.item())\n",
    "        alpha_history.append(alpha)\n",
    "        sigma_history.append(sigma)\n",
    "        \n",
    "        if np.isnan(log_marginal_likelihood.item()):\n",
    "            # if LML is NaN, then something went wrong. time to investigate\n",
    "            print('Something broke')\n",
    "            return z, alpha, sigma, likelihood_history, alpha_history, sigma_history\n",
    "    # return everything we need\n",
    "    return z, alpha, sigma, likelihood_history, alpha_history, sigma_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "# for each model\n",
    "for model_idx, model in enumerate([model_1, model_2, model_3]):\n",
    "    # for each dataset\n",
    "    for dataset_idx, dataset in enumerate([dataset_1, dataset_2]):\n",
    "        # run the gradient ascent\n",
    "        z_hat, alpha_hat, sigma_hat, likelihood_history, alpha_history, sigma_history = batch_grad_ascent(dataset['x'],\n",
    "                                                                             dataset['y'],\n",
    "                                                                             step_size_alpha=(dataset_idx + 1) * 1e-2,\n",
    "                                                                             step_size_sigma=(dataset_idx + 1) * 1e-2,\n",
    "                                                                             step_size_z=(dataset_idx + 1) * 1e-2,\n",
    "                                                                             phi=model,\n",
    "                                                                             )\n",
    "        results.append((z_hat, alpha_hat, sigma_hat, likelihood_history[-1], alpha_history[-1], sigma_history[-1]))\n",
    "        print(f'Model {model_idx}, dataset {dataset_idx}, likelihood: {likelihood_history[-1]}')\n",
    "        print(f'alpha {alpha_hat}, sigma {sigma_hat}, z {z_hat}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1529,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: 1, dataset: 1, z_hat: [], alpha_hat: 0.4353785095046029, sigma_hat: 354.0200529133279, maximal LML: -119.20505713229423\n",
      "\n",
      "model: 1, dataset: 2, z_hat: [], alpha_hat: 1e-16, sigma_hat: 2514048.4860248924, maximal LML: -828814.2872547597\n",
      "\n",
      "model: 2, dataset: 1, z_hat: [0.7008196507919382, 18.17007889934841], alpha_hat: 65.17658409212436, sigma_hat: 362.15753709090404, maximal LML: -108.35154482485123\n",
      "\n",
      "model: 2, dataset: 2, z_hat: [1e-16, 335.0918315259478], alpha_hat: 6.262438937758772e-05, sigma_hat: 2522296.238359275, maximal LML: -828977.8879457853\n",
      "\n",
      "model: 3, dataset: 1, z_hat: [], alpha_hat: 82.97761805260556, sigma_hat: 345.87419691751444, maximal LML: -108.20310620899798\n",
      "\n",
      "model: 3, dataset: 2, z_hat: [], alpha_hat: 1.8174113153563633e-05, sigma_hat: 2504646.1704369276, maximal LML: -828627.1295856825\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print best values separately from the running code\n",
    "[print(f'model: {1 + idx // 2}, dataset: {1 + idx % 2}, z_hat: {z},'\n",
    "       f' alpha_hat: {a}, sigma_hat: {s}, maximal LML: {l}\\n')\n",
    " for idx, (z, a, s, l, _, _) in enumerate(results)];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1466,
   "metadata": {},
   "outputs": [],
   "source": [
    "evidences = []\n",
    "\n",
    "# calculate evidences for each model/dataset combo\n",
    "for model_idx, model in enumerate([model_1, model_2, model_3]):\n",
    "    for dataset_idx, dataset in enumerate([dataset_1, dataset_2]):\n",
    "        # parameter count matters for the identity matrices\n",
    "        if model_idx == 0:\n",
    "            param_count = 6\n",
    "        else:\n",
    "            param_count = 2\n",
    "\n",
    "        N = dataset['x'].shape[0]\n",
    "        z_hat, alpha_hat, sigma_hat, likelihood_history, alpha_history, sigma_history = results[model_idx * 2 + dataset_idx]\n",
    "\n",
    "        # calculate log evidence because some of these numbers are too small for python to handle\n",
    "        log_evidence = (likelihood_history\n",
    "                        # parameter complexity\n",
    "                        + 0.5 * param_count * np.log(2 * np.pi)\n",
    "                        # prior\n",
    "                        + N * np.log((2 * np.pi) ** (-param_count / (2 * N)) * alpha_hat)\n",
    "                        + 0\n",
    "                        # rewriting A for tractability using (24) from matrix cookbook\n",
    "                        - 0.5 * N * np.log(sigma_hat)\n",
    "                        - 0.5 * param_count * np.log(alpha_hat)\n",
    "                        - 0.5 * np.log(\n",
    "                            np.linalg.det(- (1 / alpha_hat) * np.identity(param_count)\n",
    "                                          - (1 / sigma_hat) * model(dataset['x'], z_hat).T @ model(dataset['x'], z_hat)\n",
    "                                         )))\n",
    "\n",
    "        evidence = np.exp(log_evidence)  # becomes 0 for the second dataset because the evidence is so low...\n",
    "        print(f'Model {model_idx}, dataset {dataset_idx}, evidence: {evidence}')\n",
    "        evidences.append((model_idx, dataset_idx, log_evidence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1467,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(evidences, columns=['Model index', 'Dataset index', 'Evidence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1489,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 936x504 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(13, 7))\n",
    "\n",
    "plt.axes(axs[0])\n",
    "df[df['Dataset index'] == 0]['Evidence'].plot.bar()\n",
    "plt.title('Dataset 1')\n",
    "plt.xlabel('Model index')\n",
    "plt.xticks(ticks=[0, 1, 2], labels=[0, 1, 2])\n",
    "plt.ylabel('Log evidence (higher is better)')\n",
    "\n",
    "plt.axes(axs[1])\n",
    "df[df['Dataset index'] == 1]['Evidence'].plot.bar()\n",
    "plt.title('Dataset 2')\n",
    "plt.xlabel('Model index')\n",
    "plt.xticks(ticks=[0, 1, 2], labels=[0, 1, 2])\n",
    "plt.ylabel('Log evidence (higher is better)');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3c\n",
    "\n",
    "The model with the highest evidence on the second dataset is the second model (index 1), with\n",
    "\n",
    "$$\n",
    "\\phi(x, \\textbf{z}) = (\\exp(- \\dfrac{(x-1)^2}{z_1^2}), \\exp(- \\dfrac{(x-5)^2}{z_2^2}))^T\n",
    "$$\n",
    "\n",
    "and $\\textbf{z} = (z_1, z_2)^T$. The posterior mean and variance can be calculated using Bishop, 3.49 (with 3.50 and 3.51):\n",
    "\n",
    "$$\n",
    "p(\\textbf{w} | \\mathcal{D}) = \\mathcal{N}(\\textbf{w}| m_N, S_N)\n",
    "$$\n",
    "\n",
    "with\n",
    "\n",
    "$$\n",
    "m_N = S_N (S_0^{-1} m_0 + \\beta \\Phi^T \\textbf{t}\n",
    "$$\n",
    "\n",
    "$$\n",
    "S_N^{-1} = S_0^{-1} + \\beta \\Phi^T \\Phi\n",
    "$$\n",
    "\n",
    "Thus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1513,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Posterior mean: \n",
      "[[0.00000000e+00]\n",
      " [4.95580464e-06]], \n",
      "\n",
      "posterior variance: \n",
      "[[15968.2195697      0.        ]\n",
      " [    0.         15968.25921024]]\n"
     ]
    }
   ],
   "source": [
    "z_hat, alpha_hat, sigma_hat, likelihood_history, alpha_history, sigma_history = results[3]\n",
    "phi_xz = model_2(dataset_2['x'], z_hat)\n",
    "\n",
    "prior_mean = np.zeros((2, 1))\n",
    "S_N = (1 / alpha_hat) * np.identity(2) + (1 / sigma_hat) * phi_xz.T @ phi_xz\n",
    "m_N = np.linalg.inv(S_N) @ ((1 / alpha_hat) * np.identity(2) @ prior_mean + (1 / sigma_hat) * phi_xz.T @ dataset_2['y'])\n",
    "\n",
    "print(f'Posterior mean: \\n{m_N}, \\n\\nposterior variance: \\n{S_N}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
